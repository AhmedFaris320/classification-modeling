{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d0e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897481aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    def load_data(self):  \n",
    "        # Error check for file existence\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.filepath)\n",
    "            print(f\"âœ… Data loaded successfully from {self.filepath}.\")\n",
    "            return self.df\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Error: File not found at {self.filepath}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred during data loading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "777d8271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "    \n",
    "    def show(self, choice, n=5):\n",
    "        if choice == 'head':\n",
    "            return self.df.head(n)\n",
    "        elif choice == 'tail':\n",
    "            return self.df.tail(n)\n",
    "        elif choice == 'info':\n",
    "            # df.info() prints directly, so we call it and return None\n",
    "            self.df.info()\n",
    "            return None\n",
    "        elif choice == 'describe':\n",
    "            return self.df.describe()\n",
    "        elif choice == 'columns':\n",
    "            return self.df.columns.tolist()\n",
    "        elif choice == 'shape':\n",
    "            return self.df.shape\n",
    "        elif choice == 'dtypes':\n",
    "            return self.df.dtypes\n",
    "        elif choice == 'nulls':\n",
    "            return self.df.isnull().sum()\n",
    "        elif choice == 'duplicates':\n",
    "            return self.df.duplicated().sum()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid choice. Choose from: 'head', 'tail', 'info', 'describe', 'columns', 'shape', 'dtypes', 'nulls', 'duplicates'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3f1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, dataframe):\n",
    "        if not isinstance(dataframe, pd.DataFrame):\n",
    "            raise TypeError(\"dataframe must be a pandas DataFrame\")\n",
    "        self.df = dataframe.copy()\n",
    "\n",
    "    def handle_missing_values(self, strategy: str = 'drop', fill_value=None, columns=None, inplace: bool = False):\n",
    "        # ... (Your existing handle_missing_values code) ...\n",
    "        allowed = {'drop', 'fill', 'ffill', 'bfill', 'mean', 'median', 'mode'}\n",
    "        if not isinstance(strategy, str):\n",
    "            raise TypeError(\"strategy must be a string\")\n",
    "        st = strategy.lower()\n",
    "        if st not in allowed:\n",
    "            raise ValueError(f\"strategy must be one of {allowed}\")\n",
    "\n",
    "        columns = list(columns) if columns is not None else list(self.df.columns)\n",
    "        missing = [c for c in columns if c not in self.df.columns]\n",
    "        if missing:\n",
    "            raise KeyError(f\"columns not found in dataframe: {missing}\")\n",
    "\n",
    "        df = self.df if inplace else self.df.copy()\n",
    "\n",
    "        if st == 'drop':\n",
    "            df = df.dropna(subset=columns)\n",
    "\n",
    "        elif st == 'fill':\n",
    "            if isinstance(fill_value, dict):\n",
    "                df = df.fillna(value={k: v for k, v in fill_value.items() if k in columns})\n",
    "            else:\n",
    "                df[columns] = df[columns].fillna(fill_value)\n",
    "\n",
    "        elif st in ('ffill', 'bfill'):\n",
    "            # Only apply ffill/bfill to columns that support it\n",
    "            for c in columns:\n",
    "                if not pd.api.types.is_numeric_dtype(df[c]) and not pd.api.types.is_categorical_dtype(df[c]):\n",
    "                    print(f\"Warning: Skipping ffill/bfill on non-numeric/non-categorical column '{c}'\")\n",
    "                    continue\n",
    "                df[c] = df[c].fillna(method=st)\n",
    "\n",
    "        elif st == 'mean':\n",
    "            for c in columns:\n",
    "                if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                    df[c].fillna(df[c].mean(), inplace=True)\n",
    "\n",
    "        elif st == 'median':\n",
    "            for c in columns:\n",
    "                if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                    df[c].fillna(df[c].median(), inplace=True)\n",
    "\n",
    "        elif st == 'mode':\n",
    "            for c in columns:\n",
    "                m = df[c].mode()\n",
    "                if not m.empty:\n",
    "                    df[c].fillna(m.iloc[0], inplace=True)\n",
    "                \n",
    "        if inplace:\n",
    "            self.df = df\n",
    "            return self.df\n",
    "        return df\n",
    "\n",
    "    def remove_columns(self, columns, inplace: bool = False):\n",
    "        # ... (Your existing remove_columns code) ...\n",
    "        if isinstance(columns, str):\n",
    "            cols = [columns]\n",
    "        else:\n",
    "            cols = list(columns)\n",
    "        missing = [c for c in cols if c not in self.df.columns]\n",
    "        if missing:\n",
    "            raise KeyError(f\"columns not found in dataframe: {missing}\")\n",
    "        df = self.df if inplace else self.df.copy()\n",
    "        df = df.drop(columns=cols)\n",
    "        if inplace:\n",
    "            self.df = df\n",
    "            return self.df\n",
    "        return df\n",
    "\n",
    "    def remove_duplicates(self,inplace: bool = False):\n",
    "        # ... (Your existing remove_duplicates code) ...\n",
    "        df = self.df if inplace else self.df.copy()\n",
    "        df = df.drop_duplicates()\n",
    "        if inplace:\n",
    "            self.df = df\n",
    "            return self.df\n",
    "        return df\n",
    "\n",
    "    def encode_categorical(self, target_column=None, inplace: bool = False):\n",
    "        \"\"\"Encodes categorical columns (excluding the target if specified) using One-Hot and Label Encoding.\"\"\"\n",
    "        df = self.df if inplace else self.df.copy()\n",
    "        \n",
    "        # Identify categorical columns (excluding target)\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if target_column in categorical_cols:\n",
    "            categorical_cols.remove(target_column)\n",
    "\n",
    "        # 1. One-Hot Encode (for features)\n",
    "        df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "        print(f\"   -> One-Hot encoded features: {categorical_cols}\")\n",
    "\n",
    "        # 2. Label Encode Target (if applicable)\n",
    "        if target_column and df[target_column].dtype in ['object', 'category']:\n",
    "            le = LabelEncoder()\n",
    "            df[target_column] = le.fit_transform(df[target_column])\n",
    "            print(f\"   -> Label encoded target column: {target_column}\")\n",
    "\n",
    "        if inplace:\n",
    "            self.df = df\n",
    "            return self.df\n",
    "        return df\n",
    "    \n",
    "    def scale_numeric(self, columns=None, inplace: bool = False):\n",
    "        \"\"\"Scales numeric features using StandardScaler.\"\"\"\n",
    "        df = self.df if inplace else self.df.copy()\n",
    "        \n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Filter for specified columns if provided, otherwise use all numeric\n",
    "        if columns is not None:\n",
    "            numeric_cols = [col for col in numeric_cols if col in columns]\n",
    "\n",
    "        if not numeric_cols:\n",
    "            print(\"Warning: No numeric columns found or specified for scaling.\")\n",
    "            return df\n",
    "            \n",
    "        scaler = StandardScaler()\n",
    "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "        print(f\"   -> Scaled numeric features: {numeric_cols}\")\n",
    "\n",
    "        if inplace:\n",
    "            self.df = df\n",
    "            return self.df\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd58b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    # ... (Your existing Visualizer code) ...\n",
    "    def __init__(self, dataframe):\n",
    "        if not isinstance(dataframe, pd.DataFrame):\n",
    "            raise TypeError(\"dataframe must be a pandas DataFrame\")\n",
    "        self.df = dataframe.copy()\n",
    "\n",
    "    def show_univariate_graph(self, column, plot_type,\n",
    "                              x_label=None, y_label=None,\n",
    "                              title=None, color=None,\n",
    "                              max_categories=30, **kwargs):\n",
    "        if column not in self.df.columns:\n",
    "            print(f\"Error: Column '{column}' not found in data.\")\n",
    "            return None\n",
    "\n",
    "        data = self.df[column].dropna()\n",
    "        if data.empty:\n",
    "            print(f\"Column '{column}' has no non-missing values to plot.\")\n",
    "            return None\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "        try:\n",
    "            if plot_type == 'histogram':\n",
    "                sns.histplot(data=data, color=color, ax=ax, **kwargs)\n",
    "                ax.set_ylabel(y_label or \"Frequency / Density\")\n",
    "\n",
    "            elif plot_type == 'boxplot':\n",
    "                sns.boxplot(y=data, color=color, ax=ax, **kwargs)\n",
    "                ax.set_ylabel(y_label or \"\")\n",
    "\n",
    "            elif plot_type == 'bar':\n",
    "                if self.df[column].nunique() > max_categories:\n",
    "                    print(f\"Column '{column}' has >{max_categories} categories; showing top {max_categories}.\")\n",
    "                    top = self.df[column].value_counts().nlargest(max_categories).index\n",
    "                    sns.countplot(data=self.df[self.df[column].isin(top)], x=column, color=color, ax=ax, **kwargs)\n",
    "                else:\n",
    "                    sns.countplot(data=self.df, x=column, color=color, ax=ax, **kwargs)\n",
    "                ax.set_ylabel(y_label or \"Count\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Error: Plot type '{plot_type}' not supported. Choose from: ['histogram','boxplot','bar']\")\n",
    "                plt.close(fig)\n",
    "                return None\n",
    "\n",
    "            if x_label:\n",
    "                ax.set_xlabel(x_label)\n",
    "            if title:\n",
    "                ax.set_title(title)\n",
    "            else:\n",
    "                ax.set_title(f\"{plot_type.capitalize()} of {column}\")\n",
    "\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            return ax\n",
    "\n",
    "        except Exception as e:\n",
    "            plt.close(fig)\n",
    "            print(f\"An error occurred during plotting: {type(e).__name__}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def show_bivariate_graph(self, x_column, y_column, plot_type,\n",
    "                             x_label=None, y_label=None, title=None,\n",
    "                             color=None, hue=None, max_points=1000, **kwargs):\n",
    "        \n",
    "        if x_column not in self.df.columns or y_column not in self.df.columns:\n",
    "            print(f\"Error: Columns '{x_column}' and/or '{y_column}' not found in data.\")\n",
    "            return None\n",
    "\n",
    "        cols = [x_column, y_column]\n",
    "        if hue and hue in self.df.columns:\n",
    "            cols.append(hue)\n",
    "        elif hue and hue not in self.df.columns:\n",
    "             print(f\"Warning: Hue column '{hue}' not found and will be ignored.\")\n",
    "             hue = None\n",
    "\n",
    "\n",
    "        df_clean = self.df[cols].dropna()\n",
    "        if df_clean.empty:\n",
    "            print(f\"No non-missing pairs to plot for '{x_column}' vs '{y_column}'.\")\n",
    "            return None\n",
    "\n",
    "        if plot_type == 'scatter' and df_clean.shape[0] > max_points:\n",
    "            df_plot = df_clean.sample(max_points, random_state=42)\n",
    "        else:\n",
    "            df_plot = df_clean\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(9, 6))\n",
    "        try:\n",
    "            if plot_type == 'scatter':\n",
    "                sns.scatterplot(data=df_plot, x=x_column, y=y_column, hue=hue, ax=ax, color=color, **kwargs)\n",
    "\n",
    "            elif plot_type == 'hex':\n",
    "                sns.histplot(data=df_plot, x=x_column, y=y_column, bins=30, cmap='viridis', cbar=True, ax=ax, **kwargs)\n",
    "\n",
    "            elif plot_type == 'reg':\n",
    "                sns.regplot(data=df_plot, x=x_column, y=y_column, scatter_kws={'s': 20}, line_kws={'color': 'red'}, ax=ax, **kwargs)\n",
    "\n",
    "            elif plot_type == 'box':\n",
    "                sns.boxplot(data=df_plot, x=x_column, y=y_column, hue=hue, palette=None if color is None else [color], ax=ax, **kwargs)\n",
    "\n",
    "            elif plot_type == 'violin':\n",
    "                sns.violinplot(data=df_plot, x=x_column, y=y_column, hue=hue, palette=None if color is None else [color], ax=ax, **kwargs)\n",
    "\n",
    "            else:\n",
    "                print(f\"Error: Plot type '{plot_type}' not supported. Choose from: ['scatter','hex','reg','box','violin']\")\n",
    "                plt.close(fig)\n",
    "                return None\n",
    "\n",
    "            if x_label:\n",
    "                ax.set_xlabel(x_label)\n",
    "            if y_label:\n",
    "                ax.set_ylabel(y_label)\n",
    "            if title:\n",
    "                ax.set_title(title)\n",
    "            else:\n",
    "                ax.set_title(f\"{plot_type.capitalize()} of {y_column} vs {x_column}\")\n",
    "\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            return ax\n",
    "\n",
    "        except Exception as e:\n",
    "            plt.close(fig)\n",
    "            print(f\"An error occurred during bivariate plotting: {type(e).__name__}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55230d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparer:\n",
    "    # ... (Your existing DataPreparer code) ...\n",
    "    def __init__(self, X, y, test_size=0.3, random_state=42):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def split_data(self):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=self.test_size, random_state=self.random_state\n",
    "        )\n",
    "        print(\"âœ… Data split successfully.\")\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b5d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    # ... (Your existing ModelTrainer code) ...\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.models = {\n",
    "            \"Logistic Regression\": LogisticRegression(solver='liblinear', random_state=42, max_iter=1000), # Increased max_iter for safety\n",
    "            \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "            \"Support Vector Machine\": SVC(random_state=42, probability=True)\n",
    "        }\n",
    "        self.trained_models = {}\n",
    "\n",
    "    def train_models(self):\n",
    "        print(\"\\nðŸ› ï¸ Starting model training...\")\n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                model.fit(self.X_train, self.y_train)\n",
    "                self.trained_models[name] = model\n",
    "                print(f\"   -> {name} trained.\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error training {name}: {type(e).__name__} - {e}\")\n",
    "        print(\"âœ… All models training attempted.\")\n",
    "        return self.trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba176e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    # ... (Your existing ModelEvaluator code) ...\n",
    "    def __init__(self, trained_models, X_test, y_test):\n",
    "        self.trained_models = trained_models\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.results = {}\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        print(\"\\nðŸ“Š Starting model evaluation...\")\n",
    "        for name, model in self.trained_models.items():\n",
    "            try:\n",
    "                y_pred = model.predict(self.X_test)\n",
    "                accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                report = classification_report(self.y_test, y_pred, output_dict=True, zero_division=0)\n",
    "                \n",
    "                self.results[name] = {\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"y_pred\": y_pred,\n",
    "                    \"report\": report\n",
    "                }\n",
    "                print(f\"   -> {name} evaluated. Accuracy: {accuracy:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error evaluating {name}: {type(e).__name__} - {e}\")\n",
    "                \n",
    "        print(\"âœ… All models evaluated.\")\n",
    "        return self.results\n",
    "\n",
    "    def find_best_model(self):\n",
    "        best_model_name = \"\"\n",
    "        highest_accuracy = -1\n",
    "        \n",
    "        # Error check: ensure results are not empty\n",
    "        if not self.results:\n",
    "            print(\"\\nâš ï¸ No models were successfully evaluated.\")\n",
    "            return None, None\n",
    "            \n",
    "        for name, result in self.results.items():\n",
    "            if result['accuracy'] > highest_accuracy:\n",
    "                highest_accuracy = result['accuracy']\n",
    "                best_model_name = name\n",
    "                \n",
    "        print(\"\\n--- Model Performance Summary ---\")\n",
    "        for name, result in self.results.items():\n",
    "             print(f\"{name}: Accuracy = {result['accuracy']:.4f}\")\n",
    "             \n",
    "        print(\"\\nðŸ† **Suggested Best Model:**\")\n",
    "        print(f\"The **{best_model_name}** achieved the highest accuracy of **{highest_accuracy:.4f}**.\")\n",
    "        return best_model_name, highest_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00674c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_discrete_target(df, target_column):\n",
    "    \"\"\"Module-level helper to ensure a target column is discrete for classification.\n",
    "\n",
    "    Same heuristics as before but available for users to call directly.\n",
    "    Returns the modified DataFrame (or original if no changes applied).\n",
    "    \"\"\"\n",
    "    if target_column not in df.columns:\n",
    "        raise KeyError(f\"Target column '{target_column}' not found in DataFrame.\")\n",
    "\n",
    "    y = df[target_column]\n",
    "    print(f\"-> Inspecting target '{target_column}': dtype={y.dtype}, uniques={y.nunique()}\")\n",
    "\n",
    "    # Case: non-numeric (strings/categories)\n",
    "    if pd.api.types.is_object_dtype(y) or pd.api.types.is_categorical_dtype(y):\n",
    "        print(\"-> Target appears categorical (object/category). Applying LabelEncoder.\")\n",
    "        le = LabelEncoder()\n",
    "        df[target_column] = le.fit_transform(y.astype(str))\n",
    "        return df\n",
    "\n",
    "    # Case: numeric\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        unique = y.dropna().unique()\n",
    "        nunique = len(unique)\n",
    "\n",
    "        # integer-like small number of uniques -> cast\n",
    "        frac_max = 0\n",
    "        if y.dropna().size:\n",
    "            frac_max = (y.dropna() - y.dropna().round()).abs().max()\n",
    "        if nunique <= 10 and frac_max == 0:\n",
    "            print(\"-> Numeric target is integer-like with few unique values. Casting to integer labels.\")\n",
    "            df[target_column] = y.astype('Int64')\n",
    "            return df\n",
    "\n",
    "        # many values but many zeros (common for count targets) -> binarize\n",
    "        zeros = (y == 0).sum() if y.size else 0\n",
    "        if nunique > 10 and zeros > 0 and zeros / max(1, len(y)) > 0.2:\n",
    "            print(\"-> Many unique numeric values but a large zero mass detected. Binarizing to (y>0).\")\n",
    "            df[target_column] = (y > 0).astype('Int64')\n",
    "            return df\n",
    "\n",
    "        # rounding reduces uniques to small set -> round and cast\n",
    "        rounded = pd.Series(unique).round().unique()\n",
    "        if len(rounded) <= 10:\n",
    "            print(\"-> Rounding numeric target and casting to integer labels.\")\n",
    "            df[target_column] = y.round().astype('Int64')\n",
    "            return df\n",
    "\n",
    "    # fallback: leave as-is\n",
    "    print(\"-> No safe automatic conversion applied to target; leave unchanged.\")\n",
    "    return df\n",
    "       \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example interactive workflow (no pipeline class). Create objects and call methods manually.\n",
    "    FILE = \"C:\\\\Users\\\\Computer\\\\OneDrive\\\\Desktop\\\\BSai projects\\\\heart.csv\"\n",
    "    TARGET = 'Target'\n",
    "\n",
    "    # 1) Load\n",
    "    dl = Dataloader(FILE)\n",
    "    df = dl.load_data()\n",
    "\n",
    "    # 2) Inspect\n",
    "    analyzer = DataAnalyzer(df)\n",
    "   \n",
    "    print(analyzer.show('head', n=5))\n",
    "    \n",
    "    print(analyzer.show('describe'))\n",
    "\n",
    "    # 3) Preprocess\n",
    "    pre = Preprocessor(df)\n",
    "    df = pre.remove_duplicates()\n",
    "    df = pre.handle_missing_values(strategy='drop')\n",
    "    # ensure target is discrete for classification\n",
    "    df = ensure_discrete_target(df, TARGET)\n",
    "    # encode categorical features (skips target)\n",
    "    df = pre.encode_categorical(target_column=TARGET)\n",
    "    # scale features (exclude target)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != TARGET]\n",
    "    df = pre.scale_numeric(columns=numeric_cols)\n",
    "\n",
    "    # 4) Visualize (examples)\n",
    "    viz = Visualizer(df)\n",
    "    # show univariate\n",
    "    _ = viz.show_univariate_graph(column=TARGET, plot_type='bar', title='Target distribution')\n",
    "    # bivariate example: pick first numeric column vs target\n",
    "    nums = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(nums) >= 1 and TARGET in df.columns:\n",
    "        example_x = nums[0]\n",
    "        _ = viz.show_bivariate_graph(x_column=example_x, y_column=TARGET, plot_type='scatter', title=f'{example_x} vs {TARGET}')\n",
    "\n",
    "    # 5) Modeling (manual)\n",
    "    X = df.drop(columns=[TARGET])\n",
    "    y = df[TARGET]\n",
    "    data_prep = DataPreparer(X, y)\n",
    "    X_train, X_test, y_train, y_test = data_prep.split_data()\n",
    "    trainer = ModelTrainer(X_train, y_train)\n",
    "    models = trainer.train_models()\n",
    "    evaluator = ModelEvaluator(models, X_test, y_test)\n",
    "    results = evaluator.evaluate_models()\n",
    "    best_model, best_acc = evaluator.find_best_model()\n",
    "    if best_model:\n",
    "        print(f\"\\nBest model: {best_model} (acc={best_acc:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
